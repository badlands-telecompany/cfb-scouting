model <- train(y ~ ., data = df.train,
method = "xgbLinear",
trControl = TrainControl,
verbose = FALSE)
library(caret)
TrainControl <- trainControl(method="repeatedcv",
number = 10,
repeats = 4)
model <- train(y ~ ., data = df.train,
method = "xgbLinear",
trControl = TrainControl,
verbose = FALSE)
pred
library(caret)
library(RLightGBM)
model <-caretModel.LGBM()
fit <- train(y ~ ., data=df.train,
method=model,
verbosity = 0)
summary(fit)
pred <- predict(fit, df.test)
install.packages("RLightGMB")
library(caret)
library(RLightGBM)
library(gmodels)
# https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/
"
Three types of machine learning:
1.) Supervised learning
Ex. regression, decision trees,
random forest, KNN
- Used for prediction
2.) Unsupervised learning
Ex. a priori algorithm, k-means
- Used for clustering
3.) Reinfrocement learning
Ex. Markov decision process
- The machine is trained to make
specific decisions
"
# All algorithms supported by caret
names(getModelInfo())
###########################################
####### Decision Trees ####################
###########################################
"
A type of supervised learning algorithm that is
mostly used for classification problems.
Works for both categorical and continuous
dependent variables.
Divides a population into as many different
groups as possible.
"
library(rpart)
x1 <- rnorm(100)
x2 <- rnorm(100)
x3 <- rnorm(100)
y <- rnorm(100)
y[y > 0] <- 1; y[y <=0] <- 0
df <- data.frame(y, x1, x2, x3)
df.train <- df[1:75, ]
df.test <- df[76:100, ]
fit <- rpart(y ~ ., data=df.train, method="class")
summary(fit)
pred <- predict(fit, df.test)
###########################################
####### Support Vector Machine ############
###########################################
"
- A classification method
- We plot each data item as a point in
n-dimensional space
"
library(e1071)
fit <- svm(y ~ ., data = df.train)
summary(fit)
pred <- predict(fit, df.test)
###########################################
###### Naive Bayes ########################
###########################################
"
- A classification technique based on Bayes' theorem
- Assumes independence between predictors
- Particularly useful for very large datasets
"
library(e1071)
fit <- naiveBayes(y ~ ., data = df.train)
summary(fit)
pred <- predict(fit, df.test)
###########################################
####### K-Nearest Neighbor ################
###########################################
"
An example of instance-based learning, where new data
are classified based on stored, labelled instructions.
Uses a measure of similarity to determine the
k-nearest neighbors to the new data. The labels of
these neighbors are gathered and a majority vote is
used to classify the new data.
NOTE: k is usually odd to avoid ties
- Computationally expensive
"
iris <- read.csv(url("http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"),
header = FALSE)
names(iris) <- c("Sepal.Length", "Sepal.Width",
"Petal.Length", "Petal.Width",
"Species") # Add column names
# Iris scatter plot
library(ggvis)
iris %>% ggvis(~Sepal.Length, ~Sepal.Width,
fill = ~Species) %>% layer_points()
iris %>% ggvis(~Petal.Length, ~Petal.Width,
fill = ~Species) %>% layer_points()
# Normalizing data helps the KNN algorithm learn
normalize <- function(x) {
num <- x - min(x)
denom <- max(x) - min(x)
return (num/denom)
}
# Normalize the `iris` data
iris_norm <- as.data.frame(lapply(iris[1:4], normalize))
# Split into training and testing
set.seed(1234)
ind <- sample(x=2, size=nrow(iris), replace=TRUE,
prob=c(0.67, 0.33))
iris.training <- iris[ind==1, 1:4]
iris.test <- iris[ind==2, 1:4]
# Compose `iris` training labels
iris.trainLabels <- iris[ind==1,5]
# Compose `iris` test labels
iris.testLabels <- iris[ind==2, 5]
# The actual KNN model
# USING CARET LIBRARY
library(caret)
# Create index to split based on labels
index <- createDataPartition(iris$Species, p=0.75, list=FALSE)
iris.training <- iris[index,] # Subset training set with index
iris.test <- iris[-index,] # Subset test set with index
model_knn <- train(iris.training[, 1:4],
iris.training[, 5],
method='knn')
# USING CLASS LIBRARY
library(class)
iris_pred <- knn(train = iris.training,
test = iris.test,
cl = iris.trainLabels,
k=5) # choose k
# Evaluate model
# Compare results visually
irisTestLabels <- data.frame(iris.testLabels)
merged <- data.frame(iris_pred, iris.testLabels)
names(merged) <- c("Predicted Species",
"Observed Species"); merged
# Create a contingency table
library(gmodels)
CrossTable(x = iris.testLabels,
y = iris_pred,
prop.chisq=FALSE)
# Another way to do it
table(irisTestLabels, iris_pred)
##############################################
####### K-Means ##############################
##############################################
"
- An unsupervised algorithm for clustering problems
1.) K-means picks k number of points for each cluster
known as centroids.
2.) Each data point forms a cluster with the closest
centroids i.e. k clusters.
3.) Finds the centroid of each cluster based on
existing cluster members. Here we have new
centroids.
4.) As we have new centroids, repeat step 2 and 3.
Find the closest distance for each data point
from new centroids and get associated with new
k-clusters. Repeat this process until
convergence occurs i.e. centroids do not change.
- Find k by seeing at which point sum of squares
stabilizes
"
library(cluster)
fit <- kmeans(df, 3) # 5 cluster solution
summary(fit)
###############################################
####### Random Forest #########################
###############################################
"
Trademark term for an ensemble of decision trees
- Take a collection of decision trees.
- Each tree 'votes' for a classification
- The forest chooses the classification with the
most votes
"
library(randomForest)
fit <- randomForest(y ~ . , data=df.train,
ntree=500)
summary(fit)
pred <- predict(fit, df.test)
###############################################
### Dimensionality Reduction Algorithms #######
###############################################
"
- Can help identify relevant predictor variables
out of many
"
library(stats)
pca <- princomp(df.train, cor = TRUE)
reduced.train  <- predict(pca, df.train)
reduced.test  <- predict(pca, df.test)
###############################################
##### Gradient Boosting Algorithms ############
###############################################
##############
########## GBM
##############
"
Boosting algorithm usefel when dealing with lots of
data to make a prediction with high predictive power.
- An ensemble of learning algorithms which combines
the predictions of several base estimators
"
library(caret)
# Fitting model
fitControl <- trainControl(method = "repeatedcv",
number = 4,
repeats = 4)
fit <- train(y ~ ., data = df.train, method = "gbm",
trControl = fitControl,
verbose = FALSE)
pred <- predict(fit, df.test, type= "prob")[,2]
##############
###### XGBoost
##############
"
Immensely high predictive power; best choice for
accuracy
Possesses both linear model and the tree learning
algorithm, making the algorithm almost 10x faster
than existing gradient booster techniques
Helps to reduce overfit modelling
"
library(caret)
TrainControl <- trainControl(method="repeatedcv",
number = 10,
repeats = 4)
model <- train(y ~ ., data = df.train,
method = "xgbLinear",
trControl = TrainControl,
verbose = FALSE)
# OR
model <- train(y ~ ., data = df.train,
method = "xgbTree",
trControl = TrainControl,
verbose = FALSE)
pred <- predict(model, df.test)
# A few other gradient boosting algorithms, namely
# LightGMB and CatBoost, can be found here:
# https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/
table(irisTestLabels, iris_pred)
mean(irisTestLabels == iris_pred)
iris_pred <- knn(train = iris.training,
test = iris.test,
cl = iris.trainLabels,
k=5) # choose k
index
library(caret)
# Create index to split based on labels
index <- createDataPartition(iris$Species, p=0.75, list=FALSE)
iris.training <- iris[index,] # Subset training set with index
iris.test <- iris[-index,] # Subset test set with index
model_knn <- train(iris.training[, 1:4],
iris.training[, 5],
method='knn')
model_knn
iris.training
iris.test
iris.trainLabels
iris_pred <- knn(train = iris.training,
test = iris.test,
cl = iris.trainLabels,
k=5) # choose k
length(iris.training)
length(iris.test)
length(iris.trainLabe;s)
length(iris.trainLabe;ls)
length(iris.trainLabels)
set.seed(1234)
ind <- sample(x=2, size=nrow(iris), replace=TRUE,
prob=c(0.67, 0.33))
iris.training <- iris[ind==1, 1:4]
iris.test <- iris[ind==2, 1:4]
# Compose `iris` training labels
iris.trainLabels <- iris[ind==1,5]
# Compose `iris` test labels
iris.testLabels <- iris[ind==2, 5]
library(class)
iris_pred <- knn(train = iris.training,
test = iris.test,
cl = iris.trainLabels,
k=5) # choose k
table(irisTestLabels, iris_pred)
mean(irisTestLabels == iris_pred)
iris_pred
length(iris_pred)
length(irisTestLabels)
irisTestLabels
head(iris)
iris.testLabels <- as.vector(iris[ind==2, 5])
lengthtable(irisTestLabels, iris_pred)
table(irisTestLabels, iris_pred)
length(irisTestLabels)
irisTiris.trainLabels <- iris[ind==1,5][, 1]
iris.trainLabels <- iris[ind==1,5][, 1]
iris.testLabels <- iris[ind==2, 5]
iris.testLabels
length(iris.testLabels)
iris.testLabels <- iris[ind==2, 5]
table(irisTestLabels, iris_pred)
mean(irisTestLabels == iris_pred)
iris.trainLabels <- iris[ind==1,5]
# Compose `iris` test labels
iris.testLabels <- iris[ind==2, 5]
library(class)
iris_pred <- knn(train = iris.training,
test = iris.test,
cl = iris.trainLabels,
k=5) # choose k
# Evaluate model
table(irisTestLabels, iris_pred)
mean(irisTestLabels == iris_pred)
iris.testLabels <- iris[ind==2, 5]
iris.testLabels <- iris[ind==2, 5]
library(class)
iris_pred <- knn(train = iris.training,
test = iris.test,
cl = iris.trainLabels,
k=5) # choose k
# Evaluate model
table(iris.TestLabels, iris_pred)
mean(iris.TestLabels == iris_pred)
iris.testLabels <- iris[ind==2, 5]
library(class)
iris_pred <- knn(train = iris.training,
test = iris.test,
cl = iris.trainLabels,
k=5) # choose k
# Evaluate model
table(iris.testLabels, iris_pred)
mean(iris.testLabels == iris_pred)
iris.training <- iris[ind==1, 1:4]
iris.test <- iris[ind==2, 1:4]
# Compose `iris` training labels
iris.trainLabels <- iris[ind==1,5]
# Compose `iris` test labels
iris.testLabels <- iris[ind==2, 5]
library(class)
iris_pred <- knn(train = iris.training,
test = iris.test,
cl = iris.trainLabels,
k=5) # choose k
iris.training
iris.test
iris.trainLabels
?kmeans
fit <- kmeans(df, centers=3, nstart=20) # 3 cluster solution
summary(fit)
fit$cluster
df
plot(x$x1, x$x2, col=fit$cluster)
plot(df$x1, df$x2, col=fit$cluster)
summary(fit)
fit$withinss
print(fit)
wss <- 0
# For 1 to 15 cluster centers
for (i in 1:15) {
km.out <- kmeans(df, centers = i, nstart=20)
# Save total within sum of squares to wss variable
wss[i] <- km.out$tot.withinss
}
# Plot total within sum of squares vs. number of clusters
plot(1:15, wss, type = "b",
xlab = "Number of Clusters",
ylab = "Within groups sum of squares")
install.packages("keras")
install.packages("tfestimators")
install.packages("tensorflow")
install.packages("tfdatasets")
1:6 %in% 0:36
all(1:6 %in%0:36)
"e" %in% "abcdefg"
which(1:6 %in% 0:36)
which(30:40 %in% 0:36)
which(c("e", "d") %in% c("a", "b", "c", "d", "e", "f"))
num <- 100
char <- "character"
log <- TRUE
comp <- 1i
vec <- c(1, 2, 3)
mode(num) ; mode(char); mode(log); mode(comp)
length(num); length(vec)
pbinom(4, 8, 0.7)
?pbinom
pbinom(2, 6, 0.7)
1 - (pbinom(0, 8, 0.7) + pbinom(1, 8, 0.7))
0.1941044 / 0.9986441
dbinom(2, 6, 0.7)
dbinom(4, 8, 0.7)
1 - (binom(0, 8, 0.7) + dbinom(1, 8, 0.7))
1 - (dbinom(0, 8, 0.7) + dbinom(1, 8, 0.7))
0.1361367 / 0.9987097
1 - (dbinom(0, 10, 9/16) + dbinom(1, 10, 9/16))
1-pnorm(1.336)
1-pnorm(1.33630621)
qnorm(0.33333333333)
shiny::runApp('GitHub/cfb-scouting/shiny-stats')
runApp('GitHub/cfb-scouting/shiny-stats')
runApp('GitHub/cfb-scouting/shiny-stats')
runApp('GitHub/cfb-scouting/shiny-stats')
runApp('GitHub/cfb-scouting/shiny-stats')
x11
x11 <- 89
x11.mean <- (89 + 72 + 94 + 69) / 4
x11.range <- 94 - 69
(x11 - x11.mean) / x11.range
setwd('C:/Users/fgsci/OneDrive/Documents/GitHub/cfb-scouting')
library(data.table)
passing <- fread('Data/NFL/passing.csv')
rush_receiving <- fread('Data/NFL/rush_receiving.csv')
defense <- fread('Data/NFL/defense.csv')
setwd('C:/Users/fgsci/OneDrive/Documents/GitHub/cfb-scouting')
library(data.table)
passing <- fread('Data/NFL/passing.csv')
rush_receiving <- fread('Data/NFL/rush_receiving.csv')
defense <- fread('Data/NFL/defense.csv')
View(passing)
View(subset(passing, Name == 'Baker Mayfield'))
test <- aggregate(GP ~ Name, passing, sum)
View(test)
p.GP <- aggregate(GP ~ Name, passing, sum)
p.GS <- aggregate(GS ~ Name, passing, sum)
r.GP <- aggregate(GP ~ Name, rush_receiving, sum)
r.GS <- aggregate(GS ~ Name, rush_receiving, sum)
d.GP <- aggregate(GP ~ Name, defense, sum)
d.GS <- aggregate(GS ~ Name, defense, sum)
passing$sumGP <- with(passing, ave(GP, Name, FUN=sum))
View(passing)
setwd('C:/Users/fgsci/OneDrive/Documents/GitHub/cfb-scouting')
library(data.table)
passing <- fread('Data/NFL/passing.csv')
rush_receiving <- fread('Data/NFL/rush_receiving.csv')
defense <- fread('Data/NFL/defense.csv')
"
p.GP <- aggregate(GP ~ Name, passing, sum)
p.GS <- aggregate(GS ~ Name, passing, sum)
r.GP <- aggregate(GP ~ Name, rush_receiving, sum)
r.GS <- aggregate(GS ~ Name, rush_receiving, sum)
d.GP <- aggregate(GP ~ Name, defense, sum)
d.GS <- aggregate(GS ~ Name, defense, sum)
"
passing$sumGP <- with(passing, ave(GP, Name, FUN=sum))
rush_receiving$sumGP <- with(rush_receiving, ave(GP, Name, FUN=sum))
defense$sumGP <- with(defense, ave(GP, Name, FUN=sum))
View(rush_receiving)
passing$sumGS <- with(passing, ave(GS, Name, FUN=sum))
rush_receiving$sumGS <- with(rush_receiving, ave(GS, Name, FUN=sum))
defense$sumGS <- with(defense, ave(GS, Name, FUN=sum))
View(rush_receiving)
View(passing)
setwd('C:/Users/fgsci/OneDrive/Documents/GitHub/cfb-scouting')
library(data.table)
passing <- fread('Data/NFL/passing.csv')
rush_receiving <- fread('Data/NFL/rush_receiving.csv')
defense <- fread('Data/NFL/defense.csv')
"
p.GP <- aggregate(GP ~ Name, passing, sum)
p.GS <- aggregate(GS ~ Name, passing, sum)
r.GP <- aggregate(GP ~ Name, rush_receiving, sum)
r.GS <- aggregate(GS ~ Name, rush_receiving, sum)
d.GP <- aggregate(GP ~ Name, defense, sum)
d.GS <- aggregate(GS ~ Name, defense, sum)
"
passing$sumGP <- with(passing, ave(GP, `Player ID`, FUN=sum))
rush_receiving$sumGP <- with(rush_receiving, ave(GP, `Player ID`, FUN=sum))
defense$sumGP <- with(defense, ave(GP, `Player ID`, FUN=sum))
passing$sumGS <- with(passing, ave(GS, `Player ID`, FUN=sum))
rush_receiving$sumGS <- with(rush_receiving, ave(GS, `Player ID`, FUN=sum))
defense$sumGS <- with(defense, ave(GS, `Player ID`, FUN=sum))
View(rush_receiving)
passing$minYear <- with(passing, ave(Year, `Player ID`, FUN=min))
View(passing)
RC.passing <- subset(passing, Year <= (minYear + 3))
View(RC.passing)
combine <- fread('Data/combine.csv')
View(combine)
View(passing)
combine <- fread('Data/combine.csv')
View(combine)
combine <- fread('Data/combine.csv')
View(combine)
combine <- fread('Data/combine.csv')
View(combine)
combine <- fread('Data/test.csv')
View(test)
combine <- fread('Data/combine.csv')
test <- fread('Data/test.csv')
View(test)
type(test$Year)
whcih(test$Year)
which(test$Year)
mean(test$Year)
mean(combine$Year)
